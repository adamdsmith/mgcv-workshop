Model selection
===============
author: David L Miller
css: custom.css
transition: none


What do we mean by "model selection"?
=====================================

- Term "selection"
  - Path dependence
  - Shrinkage
- Selection between models
  - Term formulation
  - Selection between structurally different models

```{r setup, include=FALSE}
library("knitr")
library("viridis")
library("ggplot2")
library("mgcv")
library("cowplot")
theme_set(theme_minimal())
opts_chunk$set(cache=TRUE, echo=FALSE)
```

Term selection
==============
type:section

Term selection via p-values
===========================

- Old paradigm -- select terms using $p$-values
- $p$-values are **approximate**

1. treat smoothing parameters as *known*
2. rely on asymptotic behaviour

($p$-values in `summary.gam()` have changed a lot over time --- all options except current default are deprecated as of `v1.18-13` (*i.e.,* ignore what's in the book!).)

Technical stuff
===============

Test of **zero-effect** of a smooth term

Default $p$-values rely on theory of Nychka (1988) and Marra & Wood (2012) for confidence interval coverage.

If the Bayesian CI have good across-the-function properties, Wood (2013a) showed that the $p$-values have:

- almost the correct null distribution
- reasonable power

Test statistic is a form of $\chi^2$ statistic, but with complicated degrees of freedom.

(RE)ML rant again...
====================

Best behaviour when smoothness selection is done using **ML**, then **REML**.

Neither of these are the default, so remember to use `method = "ML"` or `method = "REML"` as appropriate


Term selection by shrinkage
============================
type:section

Shrinkage & additional penalties
================================

- Usually can't remove a whole function when smoothing
- Penalties used act only on *range space*
- *null space* of the basis is unpenalised.

Parts of $f$ that don't have 2<sup>nd</sup> derivatives aren't penalised

$$
\int_\mathbb{R} \left( \frac{\partial^2 f(x)}{\partial^2 x}\right)^2 \text{d}x\\
$$

Double-penalty shrinkage
========================

$\mathbf{S}_j$ penalty matrix $j$, eigendecompose:

$$
\mathbf{S}_j = \mathbf{U}_j\mathbf{\Lambda}_j\mathbf{U}_j^{T}
$$

where $\mathbf{U}_j$ is a matrix of eigenvectors and $\mathbf{\Lambda}_j$ a diagonal matrix of eigenvalues (i.e., this is an eigen decomposition of $\mathbf{S}_j$).

$\mathbf{\Lambda}_j$ contains some **0**s due to the spline basis null space --- no matter how large the penalty $\lambda_j$ might get no guarantee a smooth term will be suppressed completely.


Shrinkage & additional penalties
================================

`mgcv` has two ways to penalize the null space $\Rightarrow$ term selection

- *double penalty approach* via `select = TRUE`
- *shrinkage approach* via special bases `"ts"` and `"cs"`

Marra & Wood (2011) review other options.

Double-penalty shrinkage
========================

Create a second penalty matrix from $\mathbf{U}_j$, considering only the matrix of eigenvectors associated with the zero eigenvalues

$$
\mathbf{S}_j^{*} = \mathbf{U}_j^{*}\mathbf{U}_j^{*T}
$$

Now we can fit a GAM with two penalties of the form

$$
\lambda_j \mathbf{\beta}^T \mathbf{S}_j \mathbf{\beta} + \lambda_j^{*} \mathbf{\beta}^T \mathbf{S}_j^{*} \mathbf{\beta}
$$

In practice, add `select = TRUE` to your `gam()` call

Shrinkage
=========

- Double penalty $\Rightarrow$ twice as many smoothness parameters
- Alternative is shrinkage, add small value to zero eigenvalues
- Null space terms to be shrunk at the same time

Use `s(..., bs = "ts")` or `s(..., bs = "cs")` in **mgcv**

Selecting between models
========================
type:section

GAMs are Bayesian models
========================
type:section

Bayesian models
===============

- *duh*
  - we can build Bayesian GLMs
  - see also INLA and BayesX
- `mgcv` fits Bayesian models
- penalties are prior precision matrices
- (improper) Gaussian prior on $\boldsymbol{\beta}$

Empirical Bayes...?
===================

- Improper prior derives from $\mathbf{S}_j$ not being of full rank 
  - zeroes in $\mathbf{\Lambda}_j$.
- Double penalty and shrinkage smooths make prior proper
  - **Double penalty**: no assumption as to how much to shrink the null space
  - **Shrinkage smooths**: assume null space should be shrunk less than the wiggles

Practical Bayes
===============

Marra & Wood (2011) show that the double penalty and the shrinkage smooth approaches

- performed significantly better than alternatives in terms of *predictive ability*, and
- performed as well as alternatives in terms of variable selection


AIC
===
type:section

AIC
============

- Use full likelihood of $\boldsymbol{\beta}$ *conditional* upon $\lambda_j$ is used, with the EDF replacing $k$, the number of model parameters
- This *conditional* AIC tends to select complex models, especially those with random effects, as the EDF ignores that $\lambda_j$ are estimated
- Wood et al (2015) suggests a correction that accounts for uncertainty in $\lambda_j$ (`AIC`)


Examples
========
type:section

