Model checking
============================
author: David L Miller
css: custom.css
transition: none


Outline
=================

You fitted a GAM, everything is fine, right? *Right?*

But what about?

- Smooth terms flexibility?
- Non-constant variance?
- Response distribution selection?
- Correlated covariates?


```{r setup, echo=FALSE}
library(mgcv)
library(magrittr)
library(ggplot2)
library(dplyr)
library(statmod)
source("quantile_resid.R")
```

```{r pres_setup, include=FALSE}
library(knitr)
opts_chunk$set(cache=TRUE, echo=FALSE, fig.align="center")
```

blah
====
type:section
title:none

<div class="bigqw"><i>"perhaps the most important part of applied statistical modelling"</i></div>

<small>Simon Wood, Generalized Additive Models: An Introduction in R</small>

Basis size (k)
==============

- `k` $\approx$ number of basis functions
- Set `k` per term
- e.g. `s(x, k=10)` or `s(x, y, k=100)`
- Penalty removes "extra" wigglyness
  - *up to a point!*
- (But computation is slower with bigger `k`)

```{r loaddat}
load("../data/drake_griffen/drake_griffen.RData")
```

Default basis size
====================

```{r gam_check, fig.keep="none", include=TRUE,echo=TRUE, fig.width=15}
b <- gam(Nhat ~ s(day), data=pop_happy, method="REML")
gam.check(b)
```

Increasing basis size
====================

```{r gam_check2, fig.keep="none", include=TRUE,echo=TRUE, fig.width=15}
b_50 <- gam(Nhat ~ s(day, k=50), data=pop_happy, method="REML")
gam.check(b_50)
```

Historical/philosophical note
=============================

- "Keep relationships simple and interpretable"
  - What does this mean?
  - Bias confirmation?
  - Limit model to get "clean" relationships?
- Some literature suggests "limit `k=5`" or somesuch
  - Original `gam` package for S+ had a default `k=5`
  - Coincidence?
  - (Simon Wood, pers. comm.)



Residual checks
===============
type:section

Residuals
=========

- Deal with 2 types of residuals
  - Deviance
  - Randomized quantile
- Raw residuals are just (observed - fitted)
  - Analog to R^2
  - Difficult to assess mean-variance relationship graphically
  - Need to rescale so mean-variance is constant
  
Deviance residuals
==================

- Deviance $\approx$ "R^2 for GAMs"
- Per-observation deviance $\approx$ raw resids?
- Multiply by sign of (observed-fitted)
- Should be Normal(0, 1) distributed

gam.check() plots
=============================

`gam.check()` creates 4 plots: 

1. Quantile-quantile plots of residuals
2. Histogram of residuals
3. Residuals vs. linear predictor
4. Observed vs. fitted values


Checking response distribution
==============================

- Left side of `gam.check` plots
- Examples from the Drake & Griffen data


Normal response with count data
===================================

```{r gam_gauss, echo=TRUE, results="hide"}
b <- gam(Nhat ~ s(day, k=50), data=pop_happy, method="REML")
```
```{r gam_gauss_p, results="hide", fig.width=10, fig.height=6}
gam.check(b)
```

What about a count distribution?
================================

```{r gam_quasi, echo=TRUE, results="hide"}
b_quasi <- gam(Nhat ~ s(day, k=50), data=pop_happy, method="REML", family=quasipoisson())
```
```{r gam_quasi_p, results="hide", fig.width=10, fig.height=6}
gam.check(b_quasi)
```


What about a better count distribution?
================================

```{r gam_nb, echo=TRUE, results="hide"}
b_nb <- gam(Nhat ~ s(day, k=50), data=pop_happy, method="REML", family=nb())
```
```{r gam_nb_p, results="hide", fig.width=10, fig.height=8}
gam.check(b_nb)
```


Variance relationships
======================

- Heteroskedasticity
- Do we know that the mean-variance relationship is right?
- Deviance resids should give us constant variance if model correct?
- Right column of `gam.check`:
  - residuals vs. linear prediction == cloud
  - Response vs. fitted == line-ish

Mean-variance incorrect
=======================

```{r gam_nb_p2, results="hide", fig.width=10, fig.height=8}
gam.check(b_nb)
```

Close up
========

```{r res-linear-nb, results="hide", fig.width=10, fig.height=8}
resid <- residuals(b_nb, type = "deviance")
linpred <- predict(b_nb)
plot(linpred, resid, main = "Resids vs. linear pred.", xlab = "linear predictor", 
     ylab = "residuals", cex=1.5)
```

Randomized quantile residuals
=============================

```{r qres-linear-nb, results="hide", fig.width=10, fig.height=8}
library(statmod)
b_nb$theta <- b_nb$family$getTheta(TRUE)
resid <- qresid(b_nb)
plot(linpred, resid, main = "RQ-Resids vs. linear pred.", xlab = "linear predictor", 
     ylab = "randomized quantile residuals", cex=1.5)
```


Shortcomings
=============

- Deviance resids vs. linear predictor is victim of artifacts
- Need an alternative
- "Randomised quantile residuals" (*experimental*)
  - `rqresiduals` in `statmod`
  - Exactly normal residuals ... if the model is right!
  - `rqgam.check` in `dsm` (ignore left side plots!)

These plots are just the start
==============================

- Need to go further
- Look at aggregations of residuals by other variables

```{r resids-by-pop, results="hide", fig.width=12, fig.height=7}
pop_happy$resids <- residuals(b_nb)
boxplot(resids~ID, pop_happy, main="Residuals per experimental population")
```

Residual checking as art form
=============================

- Residuals can tell you a **lot** about your model
- No general method
  - Depends on data
  - Depends on inferential goals
- Highlight model deficiencies
- Inform what to do next; which other questions are interesting

Concurvity
=============================
type:section

What is concurvity?
======================

- Nonlinear measure, similar to co-linearity

- Measures, for each smooth term, how well this term could be approximated by
  - `concurvity(model, full=TRUE)`: some combination of all other smooth terms
  - `concurvity(model, full=FALSE)`: Each of the other smooth terms in the model 
  (useful for identifying which terms are causing issues)

A demonstration
=============================


```{r concurve1,fig.width=12, fig.height=5}
library(mgcv)
set.seed(1)
n=200
alpha = 0
x1_cc = rnorm(n)
mean_constant = alpha
var_constant = alpha^2
x2_cc = alpha*x1_cc^2 - mean_constant + rnorm(n,0,1-var_constant)
par(mfrow=c(1,3))
plot(x1_cc,x2_cc)
y = 3 + cos(pi*x1_cc) + 1/(1+exp(-5*(x2_cc)))
m1 = gam(y~s(x1_cc)+s(x2_cc),method= "REML")
plot(m1,scale=0)
print("concurvity(m1)")
print(round(concurvity(m1),2))
```



A demonstration
=============================


```{r concurve2,fig.width=12, fig.height=5}
set.seed(1)
n=200
alpha = 0.33
mean_constant = alpha
var_constant = alpha^2
x1_cc = rnorm(n)
x2_cc = alpha*x1_cc^2-mean_constant + rnorm(n,0,1-var_constant)
par(mfrow=c(1,3))
plot(x1_cc,x2_cc)
y = 3 + cos(pi*x1_cc) + 1/(1+exp(-5*(x2_cc)))
m1 = gam(y~s(x1_cc)+s(x2_cc),method= "REML")
plot(m1,scale=0)
print("concurvity(m1, full=TRUE)")
print(round(concurvity(m1),2))
```


A demonstration
=============================

```{r concurve3,fig.width=12, fig.height=5}
library(mgcv)
set.seed(1)
n=200
max_val = sqrt(pi/(pi-2))
alpha = 0.66
x1_cc = rnorm(n)
mean_constant = alpha
var_constant = alpha^2
x2_cc = alpha*x1_cc^2-mean_constant + rnorm(n,0,1-var_constant)
par(mfrow=c(1,3))
plot(x1_cc,x2_cc)
y = 3 + cos(pi*x1_cc) + 1/(1+exp(-5*(x2_cc)))
m1 = gam(y~s(x1_cc)+s(x2_cc),method= "REML")
plot(m1,scale=0)
print("concurvity(m1, full=TRUE)")
print(round(concurvity(m1),2))
```


A demonstration
=============================



```{r concurve4,fig.width=12, fig.height=5}
set.seed(1)
alpha = 1
mean_constant = alpha
var_constant = alpha^2
x2_cc = alpha*x1_cc^2-mean_constant + rnorm(n,0,1-var_constant)
par(mfrow=c(1,3))
plot(x1_cc,x2_cc)
y = 3 + cos(pi*x1_cc) + 1/(1+exp(-5*(x2_cc)))
m1 = gam(y~s(x1_cc)+s(x2_cc),method= "REML")
plot(m1,scale=0)
print("concurvity(m1, full=TRUE)")
print(round(concurvity(m1),2))
par(mfrow=c(1,1))
```

Concurvity: things to remember
==============================
- Can make your model unstable to small changes
- `cor(data)` not sufficient: use the `concurvity(model)` function
- Not always obvious from plots of smooths!!



